{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_size, \n",
    "        nhead, \n",
    "    ):\n",
    "        \"\"\"\n",
    "            :param embedding_size: dimension of word embedding (d_model in paper)\n",
    "            :param nhead: number of heads in multi-head attentions (in papers, nhead=8)\n",
    "        \"\"\"\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.nhead = nhead\n",
    "        self.d_model = embedding_size // nhead\n",
    "\n",
    "        assert (self.d_model * self.nhead == self.embedding_size), \"Embedding size must be div by n_heads\"\n",
    "        \n",
    "        self.values = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_model,\n",
    "            bias=False\n",
    "        )\n",
    "        self.keys = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_model,\n",
    "            bias=False\n",
    "        )\n",
    "        self.queries = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_model,\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        self.fc_out = nn.Linear(\n",
    "            in_features=self.nhead * self.d_model,\n",
    "            out_features=self.embedding_size\n",
    "        )\n",
    "        \n",
    "    def forward(self, values, keys, queries, mask):\n",
    "        N = queries.shape[0] # batch_size\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "\n",
    "        # Split embedding into self.heads pieces\n",
    "        values = values.reshape(N, value_len, self.nhead, self.d_model)\n",
    "        keys = keys.reshape(N, key_len, self.nhead, self.d_model)\n",
    "        queries = queries.reshape(N, query_len, self.nhead, self.d_model)\n",
    "        \n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "        \n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        # queries shape: (N, query_len, n_heads, d_model)\n",
    "        # keys shape: (N, key_len, n_heads, d_model)\n",
    "        # energy shape: (N, n_heads, query_len, key_len)\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "            # when the enery is approxminus inf, \n",
    "            # the value of energy go through softmax layer will be approx 0    \n",
    "        \n",
    "        attention = torch.softmax(energy / (self.embedding_size ** 0.5), dim=3)\n",
    "        # attention shape: (N, nhead, query_len, key_len)\n",
    "        \n",
    "        # attention shape: (N, nhead, query_len, key_len)\n",
    "        # values shape: (N, value_len, nhead, d_model)\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values])\n",
    "        # output shape: (N, query_len, nhead, d_model)\n",
    "        \n",
    "        # concat all attention head\n",
    "        out = out.reshape(N, query_len, self.nhead * self.d_model)\n",
    "        # out shape: (N, query_len, embedding_size)\n",
    "        \n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "        \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_size, \n",
    "        nhead, \n",
    "        dropout, \n",
    "        forward_expansion\n",
    "    ):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        self.attention = SelfAttention(\n",
    "            embedding_size=embedding_size,\n",
    "            nhead=nhead\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(\n",
    "            normalized_shape=embedding_size\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(\n",
    "            normalized_shape=embedding_size\n",
    "        )\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                in_features=embedding_size,\n",
    "                out_features=forward_expansion*embedding_size,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(\n",
    "                in_features=forward_expansion*embedding_size,\n",
    "                out_features=embedding_size\n",
    "            )\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        \n",
    "        x = self.norm1(attention + query)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        forward = self.feed_forward(x)\n",
    "        \n",
    "        out = self.norm2(forward + x)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        embedding_size,\n",
    "        num_layers,\n",
    "        nhead,\n",
    "        device,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(\n",
    "            num_embeddings=src_vocab_size, \n",
    "            embedding_dim=self.embedding_size\n",
    "        )\n",
    "        \n",
    "        self.position_embedding = nn.Embedding(\n",
    "            num_embeddings=max_length, \n",
    "            embedding_dim=self.embedding_size\n",
    "        )\n",
    "        \n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    self.embedding_size,\n",
    "                    nhead=nhead,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        \n",
    "        x = self.word_embedding(x) + self.position_embedding(positions)\n",
    "        out = self.dropout(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_size, \n",
    "        nhead, \n",
    "        forward_expansion, \n",
    "        dropout, \n",
    "        device\n",
    "    ):\n",
    "        \"\"\"\n",
    "        attn -> layer_norm -> transformer block\n",
    "        \"\"\"\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.attention = SelfAttention(\n",
    "            embedding_size=embedding_size,\n",
    "            nhead=nhead,\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embedding_size)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embedding_size=embedding_size,\n",
    "            nhead=nhead,\n",
    "            dropout=dropout,\n",
    "            forward_expansion=forward_expansion\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_value, enc_key, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        src_mask and tgt_mask are to hide padding values\n",
    "        value and key come from the encoder output\n",
    "        \"\"\"\n",
    "        # extract query from the input of decoder\n",
    "        attention = self.attention(\n",
    "            values=x,\n",
    "            keys=x,\n",
    "            queries=x,\n",
    "            mask=tgt_mask\n",
    "        )\n",
    "        dec_query = self.norm(attention + x)\n",
    "        dec_query = self.dropout(dec_query)\n",
    "        \n",
    "        # pass extracted query of decoder input, value and key from encoder\n",
    "        # to the transformer block\n",
    "        out = self.transformer_block(\n",
    "            value=enc_value, \n",
    "            key=enc_key, \n",
    "            query=dec_query, \n",
    "            mask=src_mask\n",
    "        )\n",
    "        return out\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tgt_vocab_size,\n",
    "        embedding_size,\n",
    "        num_layers,\n",
    "        nhead,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        device,\n",
    "        max_length,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(\n",
    "            num_embeddings=tgt_vocab_size,\n",
    "            embedding_dim=embedding_size\n",
    "        )\n",
    "        self.position_embedding = nn.Embedding(\n",
    "            num_embeddings=max_length,\n",
    "            embedding_dim=embedding_size\n",
    "        )\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(\n",
    "                    embedding_size=embedding_size,\n",
    "                    nhead=nhead,\n",
    "                    forward_expansion=forward_expansion,\n",
    "                    dropout=dropout,\n",
    "                    device=device\n",
    "                )\n",
    "                for _ in range (num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(\n",
    "            in_features=embedding_size,\n",
    "            out_features=tgt_vocab_size\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x, enc_out, src_mask, tgt_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        x = self.word_embedding(x) + self.position_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(\n",
    "                x=x,\n",
    "                enc_value=enc_out,\n",
    "                enc_key=enc_out,\n",
    "                src_mask=src_mask,\n",
    "                tgt_mask=tgt_mask\n",
    "            )\n",
    "            \n",
    "        out = self.fc_out(x)\n",
    "        return out\n",
    "        \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        tgt_vocab_size,\n",
    "        src_pad_idx,\n",
    "        tgt_pad_idx,\n",
    "        embedding_size=512,\n",
    "        num_layers=6,\n",
    "        forward_expansion=4,\n",
    "        nhead=8,\n",
    "        dropout=0,\n",
    "        device=\"cuda\",\n",
    "        max_length=128\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size=src_vocab_size,\n",
    "            embedding_size=embedding_size,\n",
    "            num_layers=num_layers,\n",
    "            nhead=nhead,\n",
    "            device=device,\n",
    "            forward_expansion=forward_expansion,\n",
    "            dropout=dropout,\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            tgt_vocab_size=tgt_vocab_size,\n",
    "            embedding_size=embedding_size,\n",
    "            num_layers=num_layers,\n",
    "            nhead=nhead,\n",
    "            forward_expansion=forward_expansion,\n",
    "            dropout=dropout,\n",
    "            device=device,\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.tgt_pad_idx = tgt_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # src_mask.shape = (N, 1, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "    \n",
    "    def make_tgt_mask(self, tgt):\n",
    "        N, tgt_len = tgt.shape\n",
    "        tgt_mask = torch.tril(torch.ones(tgt_len, tgt_len)).expand(\n",
    "            N, 1, tgt_len, tgt_len\n",
    "        )\n",
    "        return tgt_mask.to(self.device)\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        tgt_mask = self.make_tgt_mask(tgt)\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(tgt, enc_src, src_mask, tgt_mask)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gdown\n",
    "\n",
    "if os.path.exists('/content/'):\n",
    "    # os.system('!gdown 1ty8k-omlU3zvSUemx2gvBaEQWAWZAQ1C')\n",
    "    # os.system(\"!gdown \")\n",
    "    gdown.download(\"https://drive.google.com/file/d/1ty8k-omlU3zvSUemx2gvBaEQWAWZAQ1C\", output='./train.en')\n",
    "    gdown.download(\"https://drive.google.com/file/d/1mzDv83hvTlsLNSg7XNIIFLub36YVOf6u\", output='./train.vi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_token = 2\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"PAD\"}\n",
    "        self.n_words = 3  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    # return ''.join(\n",
    "    #     c for c in unicodedata.normalize('NFD', s)\n",
    "    #     if unicodedata.category(c) != 'Mn'\n",
    "    # )\n",
    "    return s\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    # s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    # s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(filename, lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    from_lang = None\n",
    "    with open(f'{filename}.{lang1}', 'r+', encoding='utf8') as f:\n",
    "        from_lang = f.read().strip().split('\\n')\n",
    "        \n",
    "    to_lang = None\n",
    "    with open(f'{filename}.{lang2}', 'r+', encoding='utf8') as f:\n",
    "        to_lang = f.read().strip().split('\\n')\n",
    "        \n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = list(zip(from_lang, to_lang))\n",
    "    pairs = [[normalizeString(s) for s in l] for l in pairs]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 133317 sentence pairs\n",
      "Trimmed to 133292 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "en 41168\n",
      "vi 18703\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 256\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "def prepareData(filename, lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(filename, lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('train', 'en', 'vi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('it would not look or feel like anything that we see when we look at a flower so if you look at this flower here and you are a little bug if you are on that surface of that flower that is what the terrain would look like',\n",
       " 'sẽ không thể nhìn hay cảm nhận bất cứ thứ gì giống như chúng ta thấy khi chúng ta nhìn một bông hoa vậy nếu bạn nhìn bông hoa ở đây và bạn là một con côn trùng bé xíu bạn ở trên bề mặt của bông hoa đó địa hình cũng giống như vậy')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [pair[0] for pair in pairs]\n",
    "Y = [pair[1] for pair in pairs]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_train[0], Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, Y, input_lang, output_lang, max_length=MAX_LENGTH):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.input_lang = input_lang\n",
    "        self.output_lang = output_lang\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src = [SOS_token] + [self.input_lang.word2index[word] for word in self.X[idx].split(' ')]\n",
    "        src.append(EOS_token)\n",
    "        src = torch.tensor(src, dtype=torch.long, device=device)\n",
    "        if len(src) < MAX_LENGTH:\n",
    "            src = torch.cat((src, torch.tensor([PAD_token] * (MAX_LENGTH - len(src)), dtype=torch.long, device=device)))\n",
    "        else:\n",
    "            src = src[:MAX_LENGTH]\n",
    "\n",
    "        tgt = [SOS_token] + [self.output_lang.word2index[word] for word in self.Y[idx].split(' ')]\n",
    "        # tgt.append(EOS_token)\n",
    "        tgt = torch.tensor([self.output_lang.word2index[word] for word in self.Y[idx].split(' ')], dtype=torch.long, device=device)\n",
    "        if len(tgt) < MAX_LENGTH:\n",
    "            tgt = torch.cat((tgt, torch.tensor([PAD_token] * (MAX_LENGTH - len(tgt)), dtype=torch.long, device=device)))\n",
    "        else:\n",
    "            tgt = tgt[:MAX_LENGTH]\n",
    "\n",
    "        return src, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,   103,    42,   270,    56,    65,   361,    43,   414,    50,\n",
      "          156,    54,    58,   156,    56,   128,     8,  9077,   192,   452,\n",
      "           46,    56,   128,    57,  9077,   165,    62,    46,    67,     8,\n",
      "          651,  4697,   452,    46,    67,    23,    50,  1686,    17,    50,\n",
      "         9077,    50,   104,   144,     5, 11851,    42,    56,    43,     1,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2], device='cuda:0') tensor([ 371,   95,  196,  310,   96,  411,  415,  360, 1148,  308,  284,  359,\n",
      "          87,  151,  434,   82,   89,  151,  434,  310,    7, 2539, 1555,  255,\n",
      "         486,   70,  310, 2539, 1555,  243,  108,   91,   70,  102,    7,  429,\n",
      "        1649, 2580,  571, 1870,   70,  243,   83,  467,  468,   39, 2539, 1555,\n",
      "         139,  291,  259,  207,  359,   87,  255,    2,    2,    2,    2,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "           2,    2,    2,    2], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(X_train, Y_train, input_lang, output_lang)\n",
    "test_dataset = Dataset(X_test, Y_test, input_lang, output_lang)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "x, y = train_dataset[0]\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SOS', 'it', 'would', 'not', 'look', 'or', 'feel', 'like', 'anything', 'that', 'we', 'see', 'when', 'we', 'look', 'at', 'a', 'flower', 'so', 'if', 'you', 'look', 'at', 'this', 'flower', 'here', 'and', 'you', 'are', 'a', 'little', 'bug', 'if', 'you', 'are', 'on', 'that', 'surface', 'of', 'that', 'flower', 'that', 'is', 'what', 'the', 'terrain', 'would', 'look', 'like', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n"
     ]
    }
   ],
   "source": [
    "print([input_lang.index2word[i.item()] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 1\n",
    "\n",
    "learning_rate = 3e-4\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "[Epoch 0 / 1]\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 0/26659 Loss: 10.0714\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 1/26659 Loss: 10.0248\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 2/26659 Loss: 9.7974\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 3/26659 Loss: 9.7332\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 4/26659 Loss: 9.4779\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 5/26659 Loss: 9.6524\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 6/26659 Loss: 9.5352\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 7/26659 Loss: 9.1540\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 8/26659 Loss: 9.1636\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 9/26659 Loss: 8.9095\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 10/26659 Loss: 8.8140\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 11/26659 Loss: 8.6064\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 12/26659 Loss: 8.2652\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 13/26659 Loss: 8.4158\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 14/26659 Loss: 8.6248\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 15/26659 Loss: 8.2716\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 16/26659 Loss: 8.2718\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 17/26659 Loss: 8.4338\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 18/26659 Loss: 8.2753\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 19/26659 Loss: 7.7209\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 20/26659 Loss: 7.9294\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 21/26659 Loss: 7.7450\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 22/26659 Loss: 7.3776\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 23/26659 Loss: 7.8470\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 24/26659 Loss: 7.8646\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 25/26659 Loss: 7.5174\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 26/26659 Loss: 6.8180\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 27/26659 Loss: 6.9125\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 28/26659 Loss: 7.2779\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 29/26659 Loss: 6.9467\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 30/26659 Loss: 7.3718\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 31/26659 Loss: 8.0528\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 32/26659 Loss: 7.7204\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 33/26659 Loss: 7.8381\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 34/26659 Loss: 7.3391\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 35/26659 Loss: 7.7727\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 36/26659 Loss: 7.0572\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 37/26659 Loss: 6.5763\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 38/26659 Loss: 6.2322\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 39/26659 Loss: 7.4264\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 40/26659 Loss: 7.2788\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 41/26659 Loss: 7.1899\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 42/26659 Loss: 7.1819\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 43/26659 Loss: 6.9968\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 44/26659 Loss: 7.5930\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 45/26659 Loss: 7.1502\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 46/26659 Loss: 7.6104\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 47/26659 Loss: 7.4814\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 48/26659 Loss: 6.9246\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 49/26659 Loss: 7.2661\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n",
      "Epoch [0/1] Batch 50/26659 Loss: 6.4672\n",
      "src: torch.Size([4, 256]), tgt: torch.Size([4, 256])\n",
      "output: torch.Size([4, 255, 18703])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\reimplementation-of-deep-learning-models\\PyTorch\\Machine Translation with Transformer\\TransformerMT.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/reimplementation-of-deep-learning-models/PyTorch/Machine%20Translation%20with%20Transformer/TransformerMT.ipynb#X15sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/reimplementation-of-deep-learning-models/PyTorch/Machine%20Translation%20with%20Transformer/TransformerMT.ipynb#X15sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     \u001b[39m# plot to tensorboard\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/reimplementation-of-deep-learning-models/PyTorch/Machine%20Translation%20with%20Transformer/TransformerMT.ipynb#X15sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m     \u001b[39m# writer.add_scalar(\"Training loss\", loss, global_step=step)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/reimplementation-of-deep-learning-models/PyTorch/Machine%20Translation%20with%20Transformer/TransformerMT.ipynb#X15sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m     \u001b[39m# step += 1\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects/reimplementation-of-deep-learning-models/PyTorch/Machine%20Translation%20with%20Transformer/TransformerMT.ipynb#X15sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m] Batch \u001b[39m\u001b[39m{\u001b[39;00mbatch_idx\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(train_loader)\u001b[39m}\u001b[39;00m\u001b[39m Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/reimplementation-of-deep-learning-models/PyTorch/Machine%20Translation%20with%20Transformer/TransformerMT.ipynb#X15sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m mean_loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(losses) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(losses)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/reimplementation-of-deep-learning-models/PyTorch/Machine%20Translation%20with%20Transformer/TransformerMT.ipynb#X15sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m scheduler\u001b[39m.\u001b[39mstep(mean_loss)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embedding_size = 512\n",
    "src_vocab_size = input_lang.n_words\n",
    "tgt_vocab_size = output_lang.n_words\n",
    "src_pad_idx = PAD_token\n",
    "tgt_pad_idx = PAD_token\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "forward_expansion = 4\n",
    "dropout = 0.1\n",
    "max_len = MAX_LENGTH\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "\n",
    "model = Transformer(\n",
    "    src_vocab_size,\n",
    "    tgt_vocab_size,\n",
    "    src_pad_idx,\n",
    "    tgt_pad_idx,\n",
    "    embedding_size,\n",
    "    num_layers,\n",
    "    forward_expansion,\n",
    "    num_heads,\n",
    "    dropout,\n",
    "    device,\n",
    "    max_len,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.1, patience=10, verbose=True\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_pad_idx)\n",
    "\n",
    "sentence = \"ein pferd geht unter einer brücke neben einem boot.\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "\n",
    "    # model.eval()\n",
    "    # translated_sentence = translate_sentence(\n",
    "    #     model, sentence, german, english, device, max_length=50\n",
    "    # )\n",
    "    # print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
    "\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Get input and targets and get to cuda\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        src, tgt = batch\n",
    "        print(f'src: {src.shape}, tgt: {tgt.shape}')\n",
    "        # Forward prop\n",
    "        input_tgt = tgt[:, :-1]\n",
    "        output = model(src, tgt[:, :-1])\n",
    "        print(f'output: {output.shape}')\n",
    "\n",
    "        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n",
    "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
    "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
    "        # way that we have output_words * batch_size that we want to send in into\n",
    "        # our cost function, so we need to do some reshapin.\n",
    "        # Let's also remove the start token while we're at it\n",
    "        output = output.reshape(-1, output.shape[-1])\n",
    "        target = tgt[:, 1:].reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        losses.append(loss.detach().cpu().item())\n",
    "\n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
    "        # within a healthy range\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        # plot to tensorboard\n",
    "        # writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "        # step += 1\n",
    "        print(f'Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(train_loader)} Loss: {loss.item():.4f}')\n",
    "\n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    scheduler.step(mean_loss)\n",
    "    break\n",
    "\n",
    "# running on entire test data takes a while\n",
    "# score = bleu(test_data[1:100], model, german, english, device)\n",
    "# print(f\"Bleu score {score * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(device)\n",
    "    src_mask = src_mask.to(device)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(device)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(device)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_token:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = [SOS_token] + [input_lang.word2index[word] for word in src_sentence.split(' ')] + [EOS_token]\n",
    "    src = torch.tensor(src).unsqueeze(1).to(device)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=SOS_token).flatten()\n",
    "    return \" \".join(output_lang.index2word[tok.item()] for tok in tgt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('vietocr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60461c33adb2bc2914643cf4ede82f2d271bf481ae2ac16ee103032447a7c5d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
